// Direct probe model on cola-pair dataset, the parameters are frozen, we only use MaskLM to do acceptability judgement 

include "../final-bert.conf"


// Do not change

word_embs_file = ""  // we do not need this in elmo, gpt or bert
fastText_model_file = ""  // we do not need this in elmo, gpt or bert

// Paths

exp_name = debug  // avoid messing up other experiment records
run_name = debug  // override exp_name and run_name when training new models

// Training setting

pretrain_tasks = ""  // we can choose pretrain the model on cola task 
do_pretrain = 0  // but not in frozen setting

target_tasks = cola-pair-frozen
do_target_task_training = 0  // do not tune the parameters on target task

transfer_paradigm = "frozen"

allow_untrained_encoder_parameters = 1  // because we test the model directly

// Model settings

bert_fine_tune = 0