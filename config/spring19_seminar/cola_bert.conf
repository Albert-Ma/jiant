// Train and run openai gpt model on cola-analysis dataset 

include "cola_elmo.conf"


// Training setting

transfer_paradigm = finetune
do_target_task_training = 0
do_full_eval = 1
batch_size = 16
lr = 1e-5
min_lr = 1e-7
optimizer = bert_adam

// Model setting

sent_enc = "null"
elmo = 0
tokenizer = "bert-base-cased"
bert_model_name = "bert-base-cased"
bert_embeddings_mode = "top"   // how to use the outputs of the BERT module
                               // set as "top", we use only the top-layer activation
                               // other options: "only" uses the lexical layer (first layer)
                               //                "cat" uses lexical layer + top layer
bert_fine_tune = 1
pair_attn = 0 // shouldn''t be needed but JIC
dropout = 0.1 // following BERT paper