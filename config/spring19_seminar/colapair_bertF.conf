// Direct probe model on cola-pair dataset, the parameters are frozen, we only use MaskLM to do acceptability judgement 
// F stands for frozen

include "../defaults.conf"


// Do not change

random_seed = 42
word_embs_file = ""  // we do not need this in elmo, gpt or bert
fastText_model_file = ""  // we do not need this in elmo, gpt or bert

// Paths

exp_name = debug  // avoid messing up other experiment records
run_name = debug  // override exp_name and run_name when training new models

// Data setting

reload_vocab = 0
reload_tasks = 0
reload_indexing = 0
max_seq_len = 80

cola = {}
cola_classifier_dropout = 0.1
cola_val_interval = 100
cola_lr = 1e-5

// Training setting

load_model = 0

pretrain_tasks = ""  // we can choose pretrain the model on cola task 
do_pretrain = 0  // but not in frozen setting

target_tasks = cola-pair-frozen
do_target_task_training = 0  // do not tune the parameters on target task
do_full_eval = 1  // evaluation the model after training

transfer_paradigm = "frozen"
batch_size = 16

allow_untrained_encoder_parameters = 1  // set this to 1 when running random-elmo
allow_reuse_of_pretraining_parameters = 0  // set this to 1 because we pretrain and finetune model on the same task

// Model settings

elmo = 0
sep_embs_for_skip = 1
tokenizer = "bert-base-uncased"
bert_model_name = "bert-base-uncased"
bert_embeddings_mode = "top"   // how to use the outputs of the BERT module
                               // set as "top", we use only the top-layer activation
                               // other options: "only" uses the lexical layer (first layer)
                               //                "cat" uses lexical layer + top layer
bert_fine_tune = 0
pair_attn = 0 // shouldn''t be needed but JIC
dropout = 0.1 // following BERT paper
sent_enc = "null"