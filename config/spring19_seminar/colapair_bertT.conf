// Train cola and run model on cola-pair dataset
// T stands for tuned

include "colapair_bertF.conf"


// Training setting

pretrain_tasks = cola
do_pretrain = 1

target_tasks = cola-pair-tuned
do_target_task_train = 0

allow_reuse_of_pretraining_parameters = 1
allow_untrained_encoder_parameters = 1

transfer_paradigm = "finetune"
max_epochs = 5 // usually 3 is enough, just to make sure
lr = 1e-5
min_lr = 1e-7
optimizer = bert_adam

lr_patience = 4  // number of epochs between last validation improvement and lr annealing, following final.conf
patience = 20  // number of epochs between last validation improvement and early stopping, following final.conf
max_vals = 10000 // following final.conf

// Model setting

bert_fine_tune = 1
classifier = log_reg // following BERT paper